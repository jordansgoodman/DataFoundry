version: "3.9"

services:
  postgres:
    image: postgres:15
    restart: unless-stopped
    labels:
      logging: promtail
      app: postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./scripts/db/init.sql:/docker-entrypoint-initdb.d/01_init.sql:ro
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 10

  redis:
    image: redis:7
    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD}"]
    restart: unless-stopped
    labels:
      logging: promtail
      app: redis
    volumes:
      - ./data/redis:/data
    networks: [df]
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10

  superset-web:
    image: apache/superset:2.1.0
    restart: unless-stopped
    depends_on: [postgres, redis]
    labels:
      logging: promtail
      app: superset-web
    env_file: .env
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      SUPERSET_ENV: production
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      DATABASE_URL: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    volumes:
      - ./data/superset:/app/superset_home
      - ./scripts/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - ./scripts/superset:/app/scripts:ro
      - ./scripts/superset:/app/scripts:ro
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8088/health || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10

  superset-worker:
    image: apache/superset:2.1.0
    restart: unless-stopped
    depends_on: [superset-web, redis]
    labels:
      logging: promtail
      app: superset-worker
    env_file: .env
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      SUPERSET_ENV: production
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      DATABASE_URL: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    command: ["bash", "-c", "celery --app=superset.tasks.celery_app:app worker -O fair -l INFO"]
    volumes:
      - ./data/superset:/app/superset_home
      - ./scripts/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - ./scripts/superset:/app/scripts:ro
      - ./scripts/superset:/app/scripts:ro
    networks: [df]

  airflow-webserver:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    restart: unless-stopped
    depends_on: [postgres, redis]
    labels:
      logging: promtail
      app: airflow-webserver
    env_file: .env
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__BASE_URL: ${AIRFLOW__WEBSERVER__BASE_URL}
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "true"
      AIRFLOW__METRICS__STATSD_ON: "true"
      AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: airflow
    volumes:
      - ./data/airflow:/opt/airflow
      - ./scripts/airflow/airflow.cfg:/opt/airflow/airflow.cfg:ro
      - ./airflow/dags:/opt/airflow/dags:ro
    command: ["bash", "-c", "airflow db init && airflow users create --username ${AIRFLOW_ADMIN_USERNAME} --password ${AIRFLOW_ADMIN_PASSWORD} --firstname Admin --lastname User --role Admin --email ${AIRFLOW_ADMIN_EMAIL} || true && airflow webserver"]
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8080/health || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10

  airflow-scheduler:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    restart: unless-stopped
    depends_on: [airflow-webserver]
    labels:
      logging: promtail
      app: airflow-scheduler
    env_file: .env
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      AIRFLOW__METRICS__STATSD_ON: "true"
      AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: airflow
    volumes:
      - ./data/airflow:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags:ro
    command: ["bash", "-c", "airflow scheduler"]
    networks: [df]

  airflow-worker:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    restart: unless-stopped
    depends_on: [airflow-webserver]
    labels:
      logging: promtail
      app: airflow-worker
    env_file: .env
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      AIRFLOW__METRICS__STATSD_ON: "true"
      AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: airflow
    volumes:
      - ./data/airflow:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags:ro
    command: ["bash", "-c", "airflow celery worker"]
    networks: [df]

  dlt:
    image: ghcr.io/dlt-hub/dlt:1.6.0
    profiles: ["tools"]
    labels:
      logging: promtail
      app: dlt
    environment:
      DLT_LOG_LEVEL: INFO
      DLT_DB_DESTINATION: postgres
      DLT_DESTINATION_POSTGRES_CREDENTIALS__HOST: postgres
      DLT_DESTINATION_POSTGRES_CREDENTIALS__PORT: 5432
      DLT_DESTINATION_POSTGRES_CREDENTIALS__DATABASE: ${POSTGRES_DB}
      DLT_DESTINATION_POSTGRES_CREDENTIALS__USERNAME: ${POSTGRES_USER}
      DLT_DESTINATION_POSTGRES_CREDENTIALS__PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./scripts/dlt:/app
    command: ["bash", "-c", "sleep infinity"]
    networks: [df]

  nginx:
    image: nginx:1.25
    restart: unless-stopped
    depends_on: [superset-web, airflow-webserver, grafana, pgadmin]
    labels:
      logging: promtail
      app: nginx
    volumes:
      - ./scripts/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "80:80"
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost/ || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10

  loki:
    image: grafana/loki:2.9.5
    restart: unless-stopped
    labels:
      logging: promtail
      app: loki
    volumes:
      - ./data/logging/loki:/loki
      - ./scripts/logging/loki-config.yml:/etc/loki/config.yml:ro
    command: ["-config.file=/etc/loki/config.yml"]
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3100/ready || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10

  promtail:
    image: grafana/promtail:2.9.5
    restart: unless-stopped
    depends_on: [loki]
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./data/logging/promtail:/promtail
      - ./scripts/logging/promtail-config.yml:/etc/promtail/config.yml:ro
    command: ["-config.file=/etc/promtail/config.yml"]
    networks: [df]

  grafana:
    image: grafana/grafana:10.4.3
    restart: unless-stopped
    depends_on: [loki, prometheus]
    labels:
      logging: promtail
      app: grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s/grafana/"
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
    volumes:
      - ./data/logging/grafana:/var/lib/grafana
      - ./scripts/logging/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./scripts/logging/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./scripts/logging/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./scripts/metrics/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources-metrics:ro
      - ./scripts/metrics/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards-metrics:ro
      - ./scripts/metrics/grafana/dashboards:/var/lib/grafana/dashboards-metrics:ro
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:3000/api/health || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10

  prometheus:
    image: prom/prometheus:v2.52.0
    restart: unless-stopped
    labels:
      logging: promtail
      app: prometheus
    volumes:
      - ./data/metrics/prometheus:/prometheus
      - ./scripts/metrics/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./scripts/metrics/alerts.yml:/etc/prometheus/alerts.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/ready || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10

  node-exporter:
    image: prom/node-exporter:v1.8.1
    restart: unless-stopped
    labels:
      logging: promtail
      app: node-exporter
    pid: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/.+)($|/)"
    networks: [df]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9100/metrics >/dev/null || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 10

  alertmanager:
    image: prom/alertmanager:v0.27.0
    restart: unless-stopped
    labels:
      logging: promtail
      app: alertmanager
    volumes:
      - ./data/metrics/alertmanager:/alertmanager
      - ./scripts/metrics/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
    networks: [df]

  pgadmin:
    image: dpage/pgadmin4:8.12
    restart: unless-stopped
    depends_on: [postgres]
    labels:
      logging: promtail
      app: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
      PGADMIN_LISTEN_PORT: 5050
    volumes:
      - ./data/pgadmin:/var/lib/pgadmin
      - ./data/pgadmin/servers.json:/pgadmin4/servers.json:ro
      - ./data/pgadmin/pgpass:/var/lib/pgadmin/pgpass:ro
    networks: [df]
  statsd-exporter:
    image: prom/statsd-exporter:v0.26.1
    restart: unless-stopped
    labels:
      logging: promtail
      app: statsd-exporter
    networks: [df]

networks:
  df: {}
